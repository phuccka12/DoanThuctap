import os
from google import genai
import language_tool_python
import textstat
import spacy
import tempfile
import whisper 
import time 
import json
import edge_tts
import asyncio
import uuid
import html  # For decoding HTML entities
from flask import Flask, request, jsonify
from flask_cors import CORS
from dotenv import load_dotenv
from werkzeug.utils import secure_filename

# --- 1. C·∫§U H√åNH H·ªÜ TH·ªêNG ---
load_dotenv()
app = Flask(__name__)
CORS(app)

api_key = os.getenv("GEMINI_API_KEY")
if not api_key:
    raise ValueError("‚ùå L·ªói: Ch∆∞a c√≥ GEMINI_API_KEY trong .env")

print("üîë ƒêang d√πng Key:", os.getenv("GEMINI_API_KEY")[:10] + "...")
# Kh·ªüi t·∫°o client v·ªõi API key
client = genai.Client(api_key=api_key)

# ‚ö†Ô∏è CH·ªåN MODEL (N·∫øu 2.5 l·ªói th√¨ ƒë·ªïi v·ªÅ 1.5-flash)
MODEL_NAME = 'gemini-2.5-flash'
print(f"üß† ƒêang k√≠ch ho·∫°t b·ªô n√£o: {MODEL_NAME}")

# --- Helper: GenAI call with retries/backoff for quota handling ---
def genai_generate_with_backoff(model, contents, max_retries=4, initial_delay=5, backoff=2):
    """Call client.models.generate_content with exponential backoff on quota/rate errors.
    Returns the response object on success or raises the last exception on failure.
    """
    delay = initial_delay
    for attempt in range(1, max_retries + 1):
        try:
            resp = client.models.generate_content(
                model=model,
                contents=contents
            )
            return resp
        except Exception as e:
            err_s = str(e)
            # Heuristic detection for quota/rate errors
            if ('RESOURCE_EXHAUSTED' in err_s) or ('429' in err_s) or ('quota' in err_s.lower()) or ('rate limit' in err_s.lower()):
                print(f"‚ö†Ô∏è GenAI quota/rate error (attempt {attempt}/{max_retries}): {err_s}")
                if attempt == max_retries:
                    print("‚ùå Max retries reached for GenAI call. Raising error.")
                    raise
                print(f"‚è≥ Backing off {delay}s before retrying GenAI call...")
                time.sleep(delay)
                delay *= backoff
                continue
            # Non-rate error: re-raise immediately
            raise

# --- 2. KH·ªûI ƒê·ªòNG C√ÅC ENGINE (QUAN TR·ªåNG: PH·∫¢I T·∫¢I H·∫æT ·ªû ƒê√ÇY) ---

# 2.1 Grammar Engine
print("‚è≥ ƒêang t·∫£i Grammar Engine (LanguageTool)...")
try:
    tool = language_tool_python.LanguageTool('en-US', remote_server='https://api.languagetool.org/v2')
except Exception as e:
    print(f"‚ö†Ô∏è L·ªói LanguageTool: {e}")
    tool = None

# 2.2 NLP Engine
print("‚è≥ ƒêang t·∫£i NLP Engine (SpaCy)...")
try:
    nlp = spacy.load("en_core_web_sm")
except:
    print("‚ö†Ô∏è ƒêang t·∫£i SpaCy model v·ªÅ m√°y...")
    os.system("python -m spacy download en_core_web_sm")
    nlp = spacy.load("en_core_web_sm")

# 2.3 Whisper Engine (S·ª¨A L·ªñI: PH·∫¢I C√ì ƒêO·∫†N N√ÄY)
print("‚è≥ ƒêang t·∫£i Whisper AI (Tai th√≠nh nh·∫•t th·∫ø gi·ªõi)...")
whisper_model = None
try:
    # Load model 'base' (c√¢n b·∫±ng t·ªëc ƒë·ªô/ch√≠nh x√°c)
    whisper_model = whisper.load_model("base")
    print("‚úÖ Whisper ƒë√£ t·∫£i th√†nh c√¥ng!")
except Exception as e:
    print(f"‚ùå L·ªñI: Kh√¥ng t·∫£i ƒë∆∞·ª£c Whisper. B·∫°n ƒë√£ c√†i FFmpeg ch∆∞a? L·ªói: {e}")


print("‚úÖ TO√ÄN B·ªò H·ªÜ TH·ªêNG ƒê√É S·∫¥N S√ÄNG CHI·∫æN ƒê·∫§U!")


# --- 3. C√ÅC H√ÄM X·ª¨ L√ù S·ªê LI·ªÜU (WRITING) ---
def analyze_deep_tech(text):
    """Ph√¢n t√≠ch s√¢u c·∫•u tr√∫c c√¢u v√† s·ªë li·ªáu"""
    doc = nlp(text)
    verbs = [token.text for token in doc if token.pos_ == "VERB"]
    sentence_starters = [sent[0].text.lower() for sent in doc.sents]
    repetitive_starters = {i:sentence_starters.count(i) for i in sentence_starters if sentence_starters.count(i) > 2}

    stats = {
        "reading_ease": textstat.flesch_reading_ease(text),
        "grade_level": textstat.text_standard(text, float_output=False),
        "verb_diversity": len(set(verbs)) / len(verbs) if verbs else 0
    }
    return {"nlp": {"starters": repetitive_starters}, "math": stats}

def check_grammar_strict(text):
    if tool is None: return [] # Tr√°nh l·ªói n·∫øu tool ch∆∞a load
    matches = tool.check(text)
    return [{"error": r.message, "context": r.context, "fix": r.replacements[:2]} for r in matches]


# ==========================================
# ‚úçÔ∏è API 1: WRITING (HYBRID ENGINE)
# ==========================================
@app.route('/api/writing/check', methods=['POST'])
def check_writing():
    try:
        data = request.json
        text = data.get('text', '')
        topic = data.get('topic', 'General Writing')
        
        if not text: return jsonify({"error": "Ch∆∞a nh·∫≠p n·ªôi dung!"}), 400

        # 1. Ch·∫°y ph√¢n t√≠ch k·ªπ thu·∫≠t
        tech_data = analyze_deep_tech(text)
        grammar_errors = check_grammar_strict(text)

        # 2. Prompt Gemini
        prompt = f"""
        # ROLE & PERSONA
        You are a Senior IELTS Examiner with 20 years of experience. You are known for being EXTREMELY STRICT and precise. You do not give high scores easily. You base your scoring on the official IELTS Writing Band Descriptors.

        # INPUT DATA
        1. TOPIC: "{topic}"
        2. STUDENT ESSAY: "{text}"
        
        # SCIENTIFIC EVIDENCE (FROM COMPUTER ANALYSIS) - DO NOT IGNORE:
        - Strict Grammar Errors Found: {len(grammar_errors)} errors. 
          (Details: {str(grammar_errors[:3])}...) -> If > 3 errors, GRA cannot be above 7.0.
        - Readability Score (Flesch): {tech_data['math']['reading_ease']} 
          (Target for Band 7+ is 30-50. If > 60, it's too simple/childish).
        - Repetitive Sentence Starters: {list(tech_data['nlp']['starters'].keys()) if tech_data['nlp']['starters'] else 'None'} 
          (If present, PENALIZE Coherence & Cohesion heavily).

        # YOUR TASK
        Analyze the essay deepy and provide a strict evaluation in VIETNAMESE.

        # STEPS TO ANALYZE:
        1. **Task Response:** Did they answer ALL parts of the prompt? Is the position clear?
        2. **Coherence:** Is the flow logical? Did they use linking words effectively or mechanically? (Check the Repetitive Starters evidence).
        3. **Lexical Resource:** Are they using 'easy' words (good, bad, nice) or 'academic' words (detrimental, beneficial)? 
        4. **Grammar:** Look at the Computer Evidence provided above. Don't be lenient.

        # OUTPUT FORMAT (JSON ONLY):
        {{
            "overall_score": "Band [Score]",
            "radar_chart": {{ "TR": [Score], "CC": [Score], "LR": [Score], "GRA": [Score] }},
            "system_feedback": [
                "M√°y t√≠nh ph√°t hi·ªán {len(grammar_errors)} l·ªói ng·ªØ ph√°p c·∫ßn s·ª≠a ngay.",
                "ƒê·ªô kh√≥ vƒÉn b·∫£n: {tech_data['math']['grade_level']} (M·ª•c ti√™u: College Level).",
                "C·∫£nh b√°o l·∫∑p t·ª´ ƒë·∫ßu c√¢u: {list(tech_data['nlp']['starters'].keys()) if tech_data['nlp']['starters'] else 'Kh√¥ng c√≥ - T·ªët'}"
            ],
            "topic_vocab_suggestion": [
                {{
                    "word": "[Advanced Word related to Topic]",
                    "meaning": "[Nghƒ©a Ti·∫øng Vi·ªát]",
                    "context": "[V√≠ d·ª• c√¢u d√πng t·ª´ n√†y thay cho t·ª´ user ƒë√£ d√πng]"
                }}
            ],
            "detailed_analysis": {{
                "task_response": "[Nh·∫≠n x√©t g·∫Øt gao v·ªÅ TR b·∫±ng Ti·∫øng Vi·ªát]",
                "coherence_cohesion": "[Nh·∫≠n x√©t v·ªÅ s·ª± m·∫°ch l·∫°c b·∫±ng Ti·∫øng Vi·ªát]",
                "lexical_resource": "[Ch√™ t·ª´ v·ª±ng ngh√®o n√†n ho·∫∑c khen t·ª´ v·ª±ng hay b·∫±ng Ti·∫øng Vi·ªát]",
                "grammar_accuracy": "[Ph√¢n t√≠ch l·ªói ng·ªØ ph√°p d·ª±a tr√™n b√°o c√°o m√°y t√≠nh b·∫±ng Ti·∫øng Vi·ªát]"
            }},
            "better_version": "[Rewrite the essay to strict Band 9.0 Standard - Academic Style]"
        }}
        """

        # G·ªçi API Gemini v·ªõi client m·ªõi (v·ªõi backoff ƒë·ªÉ tr√°nh quota errors)
        response = genai_generate_with_backoff(MODEL_NAME, prompt)
        return response.text.replace('```json', '').replace('```', '').strip(), 200

    except Exception as e:
        return jsonify({"error": str(e)}), 500


# ==========================================
# üé§ API 2: SPEAKING (WHISPER + GEMINI)
# ==========================================
@app.route('/api/speaking/check', methods=['POST'])
def check_speaking():
    try:
        start_time = time.time()
        
        # Ki·ªÉm tra Whisper c√≥ s·ªëng kh√¥ng
        if whisper_model is None:
            return jsonify({"error": "L·ªói Server: Whisper ch∆∞a ƒë∆∞·ª£c kh·ªüi ƒë·ªông (Ki·ªÉm tra FFmpeg)"}), 500

        if 'audio' not in request.files: return jsonify({"error": "No file"}), 400
        audio_file = request.files['audio']
        
        # 1. L∆∞u file
        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as tmp:
            audio_file.save(tmp.name)
            tmp_path = tmp.name

        print(f"üëÇ Whisper ƒëang nghe file: {tmp_path}...")

        # 2. WHISPER TRANSCRIBE
        result = whisper_model.transcribe(tmp_path)
        transcript = result["text"]
        print(f"üìù Transcript: {transcript}")
        
        # Cleanup
        os.remove(tmp_path)

        # 3. G·ª¨I CHO GEMINI
        uploaded_file = genai.upload_file(tmp_path)
        
        prompt = f"""
        # ROLE
        Act as a ruthless IELTS Speaking Examiner and a Phonetic Expert. Your job is to find every single mistake in the user's speech.

        # INPUT DATA
        1. AUDIO: Listen to the attached file for Intonation, Stress, and Pronunciation.
        2. VERBATIM TRANSCRIPT (From Whisper AI): "{transcript}"
           (‚ö†Ô∏è WARNING: This transcript contains EXACTLY what the user said, including fillers like 'um, ah, uh', grammar mistakes, and hesitations).

        # ANALYSIS TASKS:
        1. **Fluency & Coherence:** - Count the fillers (um, ah, uh). If there are many, score LOW.
           - Is the speed natural or robotic/too slow?
        2. **Lexical Resource:** - Are they using basic words (happy, sad, go) or idiomatic language (over the moon, devastating)?
        3. **Grammatical Range:** - Look at the TRANSCRIPT. Identify wrong tenses, wrong prepositions.
        4. **Pronunciation:** - Listen to the Audio. Identify mispronounced words compared to standard IPA.

        # OUTPUT FORMAT (JSON ONLY - Feedback in VIETNAMESE):
        {{
            "transcript_display": "{transcript}",
            "overall_score": "Band [Score]",
            "radar_chart": {{ "Fluency": [Score], "Lexical": [Score], "Grammar": [Score], "Pronunciation": [Score] }},
            "detailed_feedback": {{
                "fluency": "[Nh·∫≠n x√©t th·∫≥ng th·∫Øn v·ªÅ ƒë·ªô tr√¥i ch·∫£y, li·ªát k√™ c√°c t·ª´ ·∫≠m ·ª´]",
                "pronunciation": "[Nh·∫≠n x√©t v·ªÅ ng·ªØ ƒëi·ªáu v√† ph√°t √¢m]",
                "vocab_grammar": "[Nh·∫≠n x√©t v·ªÅ l·ªói ng·ªØ ph√°p v√† t·ª´ v·ª±ng trong Transcript]"
            }},
            "mistakes_timeline": [
                {{
                    "word": "[T·ª´ b·ªã sai/T·ª´ d·ªü]",
                    "error": "[Gi·∫£i th√≠ch t·∫°i sao sai (Grammar/Pronunciation/Choice)]",
                    "fix": "[G·ª£i √Ω s·ª≠a l·∫°i cho chu·∫©n native]"
                }}
            ],
            "vocab_upgrade": [
                {{
                    "original": "[T·ª´ v·ª±ng c∆° b·∫£n user d√πng]",
                    "better": "[T·ª´ v·ª±ng C1/C2 thay th·∫ø]",
                    "reason": "[T·∫°i sao t·ª´ m·ªõi n√†y x·ªãn h∆°n?]"
                }}
            ],
            "better_version": "[Vi·∫øt l·∫°i c√¢u tr·∫£ l·ªùi c·ªßa user theo phong c√°ch Band 9.0 t·ª± nhi√™n]"
        }}
        """

        # Upload file v√† g·ªçi API
        uploaded_file = client.files.upload(path=tmp_path)
        response = genai_generate_with_backoff(MODEL_NAME, [prompt, uploaded_file])
        
        # D·ªçn d·∫πp
        os.remove(tmp_path)
        print(f"‚úÖ X·ª≠ l√Ω xong trong {time.time() - start_time}s")

        return response.text.replace('```json', '').replace('```', '').strip(), 200

    except Exception as e:
        print(f"‚ùå Error: {str(e)}")
        return jsonify({"error": str(e)}), 500

async def generate_audio_edge(text, filepath):
    # Ch·ªçn gi·ªçng: 
    # 'en-GB-SoniaNeural': Gi·ªçng N·ªØ Anh-Anh (Chu·∫©n IELTS)
    # 'en-US-AriaNeural': Gi·ªçng N·ªØ M·ªπ
    # 'en-GB-RyanNeural': Gi·ªçng Nam Anh-Anh
    VOICE = "en-GB-SoniaNeural" 
    communicate = edge_tts.Communicate(text, VOICE)
    await communicate.save(filepath)

@app.route('/api/speaking/conversation', methods=['POST'])
def conversation():
    try:
        # Ki·ªÉm tra Whisper tr∆∞·ªõc
        if whisper_model is None: 
            return jsonify({"error": "Whisper ch∆∞a load! Ki·ªÉm tra FFmpeg."}), 500
        
        # 1. Nh·∫≠n d·ªØ li·ªáu
        if 'audio' not in request.files: 
            return jsonify({"error": "Thi·∫øu audio"}), 400
        
        audio_file = request.files['audio']
        history_str = request.form.get('history', '[]') 
        
        # 2. L∆∞u file t·∫°m & Whisper nghe
        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as tmp:
            audio_file.save(tmp.name)
            tmp_path = tmp.name

        print(f"üó£Ô∏è Conversation: Whisper ƒëang nghe...")
        
        result = whisper_model.transcribe(tmp_path)
        user_text = result["text"]
        print(f"üìù User n√≥i: {user_text}")
        os.remove(tmp_path) 

        # 3. G·ª≠i cho Gemini
        prompt = f"""
        VAI TR√í: B·∫°n l√† m·ªôt Gi√°m kh·∫£o IELTS Speaking chuy√™n nghi·ªáp.
        L·ªäCH S·ª¨: {history_str}
        C√ÇU TR·∫¢ L·ªúI M·ªöI NH·∫§T: "{user_text}"
        
        NHI·ªÜM V·ª§:
        1. Soi l·ªói ng·ªØ ph√°p/t·ª´ v·ª±ng.
        2. T·∫°o c√¢u h·ªèi ti·∫øp theo (Ti·∫øng Anh).
        3. T·∫°o l·ªùi khuy√™n s·ª≠a l·ªói (Ti·∫øng Vi·ªát).

        OUTPUT JSON:
        {{
            "examiner_response_text": "[C√¢u h·ªèi ti·∫øp theo b·∫±ng Ti·∫øng Anh]",
            "correction_tip": "[L·ªùi khuy√™n s·ª≠a l·ªói b·∫±ng Ti·∫øng Vi·ªát]",
            "is_short_answer": true/false
        }}
        """
        
        response = genai_generate_with_backoff(MODEL_NAME, prompt)
        response_json = response.text.replace('```json', '').replace('```', '').strip()
        
        # 4. X·ª≠ l√Ω JSON & T·∫°o Audio
        data = json.loads(response_json)
        ai_text = data.get("examiner_response_text", "Could you repeat that?")
        correction = data.get("correction_tip", "")

        # T·∫°o t√™n file
        filename = f"ai_ask_{uuid.uuid4()}.mp3"
        filepath = os.path.join("static", filename)
        
        # ƒê·∫£m b·∫£o th∆∞ m·ª•c static t·ªìn t·∫°i
        os.makedirs("static", exist_ok=True)
        
        # G·ªçi h√†m Edge TTS (X·ª≠ l√Ω Async)
        try:
            asyncio.run(generate_audio_edge(ai_text, filepath))
        except RuntimeError as e:
            # Fallback n·∫øu l·ªói loop
            if "cannot be called from a running event loop" in str(e):
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                loop.run_until_complete(generate_audio_edge(ai_text, filepath))
                loop.close()
            else:
                raise

        audio_url = f"{request.host_url}static/{filename}"
        
        return jsonify({
            "user_transcript": user_text,
            "ai_response_text": ai_text,
            "ai_audio_url": audio_url,
            "correction": correction
        })

    except Exception as e:
        print(f"‚ùå L·ªói Conversation: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500


# ==========================================
# ü§ñ API 4: AGENTIC CONTENT ENGINE
# Multi-Agent System: Architect ‚Üí Author ‚Üí Critic ‚Üí Self-Correction
# ==========================================

# CEFR to Flesch Reading Ease mapping
CEFR_FLESCH_MAP = {
    'A1': {'min': 70, 'max': 100, 'grade': 'Very Easy'},
    'A2': {'min': 60, 'max': 80, 'grade': 'Easy'},
    'B1': {'min': 50, 'max': 65, 'grade': 'Fairly Easy'},
    'B2': {'min': 40, 'max': 55, 'grade': 'Standard'},
    'C1': {'min': 30, 'max': 45, 'grade': 'Fairly Difficult'},
    'C2': {'min': 0, 'max': 40, 'grade': 'Difficult'}
}

def critic_audit(passage, title, cefr_level, target_word_count):
    """
    AGENT 3: The Critic - Symbolic AI Auditor
    Returns: (passed: bool, report: dict, errors: list)
    """
    errors = []
    report = {}
    
    # 1. Word Count Check
    words = passage.split()
    actual_count = len(words)
    report['word_count'] = actual_count
    tolerance = int(target_word_count * 0.25)  # ¬±25%
    if abs(actual_count - target_word_count) > tolerance:
        errors.append(f"Word count {actual_count} out of range ({target_word_count} ¬±{tolerance})")
    
    # 2. Readability Check (Flesch)
    try:
        flesch_score = textstat.flesch_reading_ease(passage)
        report['flesch_score'] = round(flesch_score, 2)
        
        target_range = CEFR_FLESCH_MAP.get(cefr_level, {'min': 40, 'max': 60})
        if flesch_score < target_range['min'] or flesch_score > target_range['max']:
            errors.append(f"Readability score {flesch_score} outside {cefr_level} range ({target_range['min']}-{target_range['max']})")
    except:
        report['flesch_score'] = None
        errors.append("Could not calculate readability score")
    
    # 3. Lexical Diversity Check
    doc = nlp(passage.lower())
    tokens = [token.text for token in doc if token.is_alpha]
    if len(tokens) > 0:
        unique_tokens = len(set(tokens))
        diversity_ratio = unique_tokens / len(tokens)
        report['lexical_diversity'] = round(diversity_ratio, 3)
        
        if diversity_ratio < 0.40:  # Too repetitive
            errors.append(f"Low lexical diversity ({diversity_ratio:.2%}), too many repeated words")
        
        # Check for overused words
        from collections import Counter
        word_freq = Counter(tokens)
        most_common = word_freq.most_common(1)[0] if word_freq else None
        if most_common and most_common[1] / len(tokens) > 0.08:
            errors.append(f"Word '{most_common[0]}' repeated {most_common[1]} times (overused)")
    
    # 4. Grammar Check (if tool available)
    if tool:
        try:
            matches = tool.check(passage)
            major_errors = [m for m in matches if m.category in ['GRAMMAR', 'TYPOS']]
            report['grammar_errors'] = len(major_errors)
            if len(major_errors) > 5:
                errors.append(f"Too many grammar errors ({len(major_errors)})")
        except:
            report['grammar_errors'] = None
    
    # 5. Structure Check
    if not title or len(title) < 5:
        errors.append("Title too short or missing")
    
    if len(passage) < 100:
        errors.append("Passage too short (< 100 characters)")
    
    passed = len(errors) == 0
    return passed, report, errors

@app.route('/api/agentic/generate-reading', methods=['POST'])
def agentic_generate_reading():
    """
    üß† AGENTIC CONTENT ENGINE
    Multi-Agent System v·ªõi Self-Correction Loop
    """
    try:
        data = request.json
        topic = data.get('topic', 'General Topic')
        cefr_level = data.get('cefr_level', 'B1')
        word_count = data.get('wordCount', 150)
        tone = data.get('tone', 'neutral')
        topic_hints = data.get('topicHints', '')
        core_vocab = data.get('core_vocab', [])
        max_retries = data.get('maxRetries', 3)
        
        print(f"\n{'='*60}")
        print(f"üéØ AGENTIC GENERATION START")
        print(f"   Topic: {topic}")
        print(f"   CEFR: {cefr_level} | Words: {word_count}")
        print(f"{'='*60}\n")
        
        attempts = 0
        final_result = None
        all_attempts = []
        
        # === PHASE 1: ARCHITECT (Planning) ===
        print("üèóÔ∏è  AGENT 1 (ARCHITECT): Creating outline...")
        architect_prompt = f"""
You are "The Architect" - a pedagogical planner for IELTS reading materials.

INPUT:
- Topic: {topic}
- CEFR Level: {cefr_level}
- Target Word Count: {word_count}
- Tone: {tone}
{f"- Topic Hints: {topic_hints}" if topic_hints else ""}

TASK: Create a structured outline for a reading passage.

OUTPUT (JSON only, no markdown):
{{
  "title_suggestion": "Engaging title here",
  "learning_objectives": ["objective 1", "objective 2"],
  "sections": [
    {{"name": "Introduction", "instructions": "Set context in 1-2 sentences"}},
    {{"name": "Body Part 1", "instructions": "Main idea development"}},
    {{"name": "Body Part 2", "instructions": "Supporting details or contrast"}},
    {{"name": "Conclusion", "instructions": "Brief closing thought"}}
  ],
  "recommended_vocab": ["word1", "word2", "word3"]
}}
"""
        
        try:
            arch_response = genai_generate_with_backoff(MODEL_NAME, architect_prompt)
            outline_text = arch_response.text.replace('```json', '').replace('```', '').strip()
            outline = json.loads(outline_text)
            
            # Decode HTML entities in outline fields
            if 'title_suggestion' in outline:
                outline['title_suggestion'] = html.unescape(outline['title_suggestion'])
            
            print(f"‚úÖ Outline created: {outline.get('title_suggestion', 'N/A')}")
        except Exception as e:
            err_s = str(e)
            print(f"‚ùå Architect failed: {err_s}")
            # If it's a quota/rate error, return 429 with friendly message
            if ('RESOURCE_EXHAUSTED' in err_s) or ('429' in err_s) or ('quota' in err_s.lower()) or ('rate limit' in err_s.lower()):
                return jsonify({
                    "error": "Architect failed due to API quota/rate limits. Please check your Gemini quota/billing or try again later.",
                    "details": err_s
                }), 429
            return jsonify({"error": f"Architect failed: {err_s}"}), 500
        
        # Merge vocab
        all_vocab = list(set(core_vocab + outline.get('recommended_vocab', [])))
        time.sleep(15)
        # === PHASE 2-4: AUTHOR + CRITIC LOOP ===
        while attempts < max_retries:
            attempts += 1
            print(f"\nüìù AGENT 2 (AUTHOR): Attempt {attempts}/{max_retries}")
            
            # üí§ DELAY ƒë·ªÉ tr√°nh rate limit (Free tier: 15 req/min = 1 req/4s)
            if attempts > 1:  # Skip delay for first attempt
                print(f"‚è≥ Waiting 5 seconds before attempt {attempts} (Free API rate limit protection)...")
                time.sleep(5)  # 5s delay = safe for Free tier
            
            # Build Author prompt
            if attempts == 1:
                # Get readability guidance
                flesch_range = CEFR_FLESCH_MAP.get(cefr_level, {'min': 40, 'max': 60, 'grade': 'Standard'})
                
                author_prompt = f"""
You are "The Author" - an expert writer of IELTS reading materials.

OUTLINE (follow this structure):
{json.dumps(outline, indent=2)}

REQUIREMENTS:
- Write exactly {word_count} words (¬±10%)
- CEFR Level: {cefr_level} - {flesch_range['grade']} 
- Tone: {tone}
- MUST include these vocabulary words naturally: {', '.join(all_vocab[:10]) if all_vocab else 'none'}

READABILITY RULES FOR {cefr_level}:
- Target Flesch Reading Ease Score: {flesch_range['min']}-{flesch_range['max']}
- Use {'VERY SHORT sentences (5-8 words). SIMPLE vocabulary only.' if cefr_level in ['A1', 'A2'] else 'short-to-medium sentences (8-15 words). Clear, common vocabulary.' if cefr_level == 'B1' else 'medium sentences (12-18 words). Some complex words OK.' if cefr_level == 'B2' else 'longer sentences (15-25 words). Advanced vocabulary encouraged.' if cefr_level in ['C1', 'C2'] else 'medium sentences'}
- Avoid: {'complex grammar, subordinate clauses, advanced idioms' if cefr_level in ['A1', 'A2', 'B1'] else 'overly academic jargon only' if cefr_level == 'B2' else 'only extremely rare words'}

IMPORTANT: Write PLAIN TEXT only. Do NOT use HTML tags, HTML entities (&nbsp;, &lt;, etc.), or any markup. Use normal spaces and punctuation.

OUTPUT (JSON only, no markdown):
{{
  "title": "Your final title",
  "passage": "Full passage text here. Multiple paragraphs separated by double newlines."
}}
"""
            else:
                # Self-Correction prompt
                prev_errors = all_attempts[-1]['errors']
                prev_flesch = all_attempts[-1]['audit_report'].get('flesch_score', 0)
                flesch_range = CEFR_FLESCH_MAP.get(cefr_level, {'min': 40, 'max': 60})
                
                author_prompt = f"""
You are "The Author". Your previous draft was REJECTED by the Critic.

REASONS FOR REJECTION:
{chr(10).join(f"- {err}" for err in prev_errors)}

PREVIOUS FLESCH SCORE: {prev_flesch} (Target: {flesch_range['min']}-{flesch_range['max']})

OUTLINE (follow this):
{json.dumps(outline, indent=2)}

CORRECTIVE ACTIONS REQUIRED:
{'- INCREASE readability: Use MUCH SIMPLER words, SHORTER sentences (8-12 words max)' if prev_flesch < flesch_range['min'] else '- DECREASE readability: Use more complex vocabulary and longer sentences' if prev_flesch > flesch_range['max'] else '- Maintain current complexity'}
- Reduce word repetition: use synonyms and varied expressions
- Fix grammar issues if any
- Adjust word count to: {word_count} ¬±10%
- Keep these vocabulary words: {', '.join(all_vocab[:10]) if all_vocab else 'none'}

SPECIFIC TIPS FOR {cefr_level}:
{'- Use present simple tense mostly' if cefr_level in ['A1', 'A2'] else '- Mix simple and continuous tenses' if cefr_level == 'B1' else '- Use varied tenses including perfect forms'}
{'- Avoid phrasal verbs and idioms' if cefr_level in ['A1', 'A2'] else '- Use common phrasal verbs sparingly' if cefr_level == 'B1' else '- Phrasal verbs and idioms OK'}
- Average sentence length: {8 if cefr_level in ['A1', 'A2'] else 12 if cefr_level == 'B1' else 15 if cefr_level == 'B2' else 18} words

IMPORTANT: Write PLAIN TEXT only. Do NOT use HTML tags, HTML entities (&nbsp;, &lt;, etc.), or any markup. Use normal spaces and punctuation.

OUTPUT (JSON only, no markdown):
{{
  "title": "Improved title",
  "passage": "Rewritten passage text."
}}
"""
            
            try:
                print(f"üñäÔ∏è  AGENT 2 (AUTHOR): Writing attempt {attempts}...")
                author_response = genai_generate_with_backoff(MODEL_NAME, author_prompt)
                draft_text = author_response.text.replace('```json', '').replace('```', '').strip()
                draft = json.loads(draft_text)
                title = draft.get('title', '')
                passage = draft.get('passage', '')
                
                # Decode HTML entities (e.g., &nbsp; ‚Üí space, &lt; ‚Üí <)
                title = html.unescape(title)
                passage = html.unescape(passage)
                
                print(f"   Generated: {len(passage.split())} words")
                
            except Exception as e:
                print(f"‚ùå Author failed: {e}")
                all_attempts.append({
                    'attempt': attempts,
                    'status': 'author_failed',
                    'error': str(e)
                })
                continue
            
            # === PHASE 3: CRITIC AUDIT ===
            print(f"üîç AGENT 3 (CRITIC): Auditing draft {attempts}...")
            passed, report, errors = critic_audit(passage, title, cefr_level, word_count)
            
            attempt_record = {
                'attempt': attempts,
                'title': title,
                'word_count': len(passage.split()),
                'audit_report': report,
                'errors': errors,
                'status': 'accepted' if passed else 'rejected'
            }
            all_attempts.append(attempt_record)
            
            if passed:
                print(f"‚úÖ ACCEPTED! (Flesch: {report.get('flesch_score', 'N/A')}, Diversity: {report.get('lexical_diversity', 'N/A')})")
                final_result = {
                    'status': 'success',
                    'attempts': attempts,
                    'title': title,
                    'passage': passage,
                    'outline': outline,
                    'audit_report': report,
                    'cefr_level': cefr_level,
                    'word_count': len(passage.split()),
                    'all_attempts': all_attempts
                }
                break
            else:
                print(f"‚ùå REJECTED: {errors}")
                if attempts >= max_retries:
                    print(f"‚ö†Ô∏è  Max retries reached. Returning best attempt.")
                    final_result = {
                        'status': 'max_retries_reached',
                        'attempts': attempts,
                        'title': title,
                        'passage': passage,
                        'outline': outline,
                        'audit_report': report,
                        'errors': errors,
                        'all_attempts': all_attempts,
                        'warning': 'Generated content did not pass all audits after max retries'
                    }
        
        if not final_result:
            return jsonify({"error": "Generation failed after all attempts"}), 500
        
        print(f"\n{'='*60}")
        print(f"üéâ GENERATION COMPLETE: {final_result['status']}")
        print(f"{'='*60}\n")
        
        return jsonify(final_result), 200
        
    except Exception as e:
        print(f"‚ùå System Error: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500


if __name__ == '__main__':
    app.run(port=5000, debug=True)