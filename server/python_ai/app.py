import os
from google import genai
import language_tool_python
import textstat
import spacy
import tempfile
import whisper 
import time 
import json
import edge_tts
import asyncio
import uuid
from flask import Flask, request, jsonify
from flask_cors import CORS
from dotenv import load_dotenv
from werkzeug.utils import secure_filename

# --- 1. C·∫§U H√åNH H·ªÜ TH·ªêNG ---
load_dotenv()
app = Flask(__name__)
CORS(app)

api_key = os.getenv("GEMINI_API_KEY")
if not api_key:
    raise ValueError("‚ùå L·ªói: Ch∆∞a c√≥ GEMINI_API_KEY trong .env")

# Kh·ªüi t·∫°o client v·ªõi API key
client = genai.Client(api_key=api_key)

# ‚ö†Ô∏è CH·ªåN MODEL (N·∫øu 2.5 l·ªói th√¨ ƒë·ªïi v·ªÅ 1.5-flash)
MODEL_NAME = 'gemini-2.5-flash'
print(f"üß† ƒêang k√≠ch ho·∫°t b·ªô n√£o: {MODEL_NAME}")

# --- 2. KH·ªûI ƒê·ªòNG C√ÅC ENGINE (QUAN TR·ªåNG: PH·∫¢I T·∫¢I H·∫æT ·ªû ƒê√ÇY) ---

# 2.1 Grammar Engine
print("‚è≥ ƒêang t·∫£i Grammar Engine (LanguageTool)...")
try:
    tool = language_tool_python.LanguageTool('en-US', remote_server='https://api.languagetool.org/v2')
except Exception as e:
    print(f"‚ö†Ô∏è L·ªói LanguageTool: {e}")
    tool = None

# 2.2 NLP Engine
print("‚è≥ ƒêang t·∫£i NLP Engine (SpaCy)...")
try:
    nlp = spacy.load("en_core_web_sm")
except:
    print("‚ö†Ô∏è ƒêang t·∫£i SpaCy model v·ªÅ m√°y...")
    os.system("python -m spacy download en_core_web_sm")
    nlp = spacy.load("en_core_web_sm")

# 2.3 Whisper Engine (S·ª¨A L·ªñI: PH·∫¢I C√ì ƒêO·∫†N N√ÄY)
print("‚è≥ ƒêang t·∫£i Whisper AI (Tai th√≠nh nh·∫•t th·∫ø gi·ªõi)...")
whisper_model = None
try:
    # Load model 'base' (c√¢n b·∫±ng t·ªëc ƒë·ªô/ch√≠nh x√°c)
    whisper_model = whisper.load_model("base")
    print("‚úÖ Whisper ƒë√£ t·∫£i th√†nh c√¥ng!")
except Exception as e:
    print(f"‚ùå L·ªñI: Kh√¥ng t·∫£i ƒë∆∞·ª£c Whisper. B·∫°n ƒë√£ c√†i FFmpeg ch∆∞a? L·ªói: {e}")


print("‚úÖ TO√ÄN B·ªò H·ªÜ TH·ªêNG ƒê√É S·∫¥N S√ÄNG CHI·∫æN ƒê·∫§U!")


# --- 3. C√ÅC H√ÄM X·ª¨ L√ù S·ªê LI·ªÜU (WRITING) ---
def analyze_deep_tech(text):
    """Ph√¢n t√≠ch s√¢u c·∫•u tr√∫c c√¢u v√† s·ªë li·ªáu"""
    doc = nlp(text)
    verbs = [token.text for token in doc if token.pos_ == "VERB"]
    sentence_starters = [sent[0].text.lower() for sent in doc.sents]
    repetitive_starters = {i:sentence_starters.count(i) for i in sentence_starters if sentence_starters.count(i) > 2}

    stats = {
        "reading_ease": textstat.flesch_reading_ease(text),
        "grade_level": textstat.text_standard(text, float_output=False),
        "verb_diversity": len(set(verbs)) / len(verbs) if verbs else 0
    }
    return {"nlp": {"starters": repetitive_starters}, "math": stats}

def check_grammar_strict(text):
    if tool is None: return [] # Tr√°nh l·ªói n·∫øu tool ch∆∞a load
    matches = tool.check(text)
    return [{"error": r.message, "context": r.context, "fix": r.replacements[:2]} for r in matches]


# ==========================================
# ‚úçÔ∏è API 1: WRITING (HYBRID ENGINE)
# ==========================================
@app.route('/api/writing/check', methods=['POST'])
def check_writing():
    try:
        data = request.json
        text = data.get('text', '')
        topic = data.get('topic', 'General Writing')
        
        if not text: return jsonify({"error": "Ch∆∞a nh·∫≠p n·ªôi dung!"}), 400

        # 1. Ch·∫°y ph√¢n t√≠ch k·ªπ thu·∫≠t
        tech_data = analyze_deep_tech(text)
        grammar_errors = check_grammar_strict(text)

        # 2. Prompt Gemini
        prompt = f"""
        # ROLE & PERSONA
        You are a Senior IELTS Examiner with 20 years of experience. You are known for being EXTREMELY STRICT and precise. You do not give high scores easily. You base your scoring on the official IELTS Writing Band Descriptors.

        # INPUT DATA
        1. TOPIC: "{topic}"
        2. STUDENT ESSAY: "{text}"
        
        # SCIENTIFIC EVIDENCE (FROM COMPUTER ANALYSIS) - DO NOT IGNORE:
        - Strict Grammar Errors Found: {len(grammar_errors)} errors. 
          (Details: {str(grammar_errors[:3])}...) -> If > 3 errors, GRA cannot be above 7.0.
        - Readability Score (Flesch): {tech_data['math']['reading_ease']} 
          (Target for Band 7+ is 30-50. If > 60, it's too simple/childish).
        - Repetitive Sentence Starters: {list(tech_data['nlp']['starters'].keys()) if tech_data['nlp']['starters'] else 'None'} 
          (If present, PENALIZE Coherence & Cohesion heavily).

        # YOUR TASK
        Analyze the essay deepy and provide a strict evaluation in VIETNAMESE.

        # STEPS TO ANALYZE:
        1. **Task Response:** Did they answer ALL parts of the prompt? Is the position clear?
        2. **Coherence:** Is the flow logical? Did they use linking words effectively or mechanically? (Check the Repetitive Starters evidence).
        3. **Lexical Resource:** Are they using 'easy' words (good, bad, nice) or 'academic' words (detrimental, beneficial)? 
        4. **Grammar:** Look at the Computer Evidence provided above. Don't be lenient.

        # OUTPUT FORMAT (JSON ONLY):
        {{
            "overall_score": "Band [Score]",
            "radar_chart": {{ "TR": [Score], "CC": [Score], "LR": [Score], "GRA": [Score] }},
            "system_feedback": [
                "M√°y t√≠nh ph√°t hi·ªán {len(grammar_errors)} l·ªói ng·ªØ ph√°p c·∫ßn s·ª≠a ngay.",
                "ƒê·ªô kh√≥ vƒÉn b·∫£n: {tech_data['math']['grade_level']} (M·ª•c ti√™u: College Level).",
                "C·∫£nh b√°o l·∫∑p t·ª´ ƒë·∫ßu c√¢u: {list(tech_data['nlp']['starters'].keys()) if tech_data['nlp']['starters'] else 'Kh√¥ng c√≥ - T·ªët'}"
            ],
            "topic_vocab_suggestion": [
                {{
                    "word": "[Advanced Word related to Topic]",
                    "meaning": "[Nghƒ©a Ti·∫øng Vi·ªát]",
                    "context": "[V√≠ d·ª• c√¢u d√πng t·ª´ n√†y thay cho t·ª´ user ƒë√£ d√πng]"
                }}
            ],
            "detailed_analysis": {{
                "task_response": "[Nh·∫≠n x√©t g·∫Øt gao v·ªÅ TR b·∫±ng Ti·∫øng Vi·ªát]",
                "coherence_cohesion": "[Nh·∫≠n x√©t v·ªÅ s·ª± m·∫°ch l·∫°c b·∫±ng Ti·∫øng Vi·ªát]",
                "lexical_resource": "[Ch√™ t·ª´ v·ª±ng ngh√®o n√†n ho·∫∑c khen t·ª´ v·ª±ng hay b·∫±ng Ti·∫øng Vi·ªát]",
                "grammar_accuracy": "[Ph√¢n t√≠ch l·ªói ng·ªØ ph√°p d·ª±a tr√™n b√°o c√°o m√°y t√≠nh b·∫±ng Ti·∫øng Vi·ªát]"
            }},
            "better_version": "[Rewrite the essay to strict Band 9.0 Standard - Academic Style]"
        }}
        """

        # G·ªçi API Gemini v·ªõi client m·ªõi
        response = client.models.generate_content(
            model=MODEL_NAME,
            contents=prompt
        )
        return response.text.replace('```json', '').replace('```', '').strip(), 200

    except Exception as e:
        return jsonify({"error": str(e)}), 500


# ==========================================
# üé§ API 2: SPEAKING (WHISPER + GEMINI)
# ==========================================
@app.route('/api/speaking/check', methods=['POST'])
def check_speaking():
    try:
        start_time = time.time()
        
        # Ki·ªÉm tra Whisper c√≥ s·ªëng kh√¥ng
        if whisper_model is None:
            return jsonify({"error": "L·ªói Server: Whisper ch∆∞a ƒë∆∞·ª£c kh·ªüi ƒë·ªông (Ki·ªÉm tra FFmpeg)"}), 500

        if 'audio' not in request.files: return jsonify({"error": "No file"}), 400
        audio_file = request.files['audio']
        
        # 1. L∆∞u file
        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as tmp:
            audio_file.save(tmp.name)
            tmp_path = tmp.name

        print(f"üëÇ Whisper ƒëang nghe file: {tmp_path}...")

        # 2. WHISPER TRANSCRIBE
        result = whisper_model.transcribe(tmp_path)
        transcript = result["text"]
        print(f"üìù Transcript: {transcript}")

        # 3. G·ª¨I CHO GEMINI
        uploaded_file = genai.upload_file(tmp_path)
        
        prompt = f"""
        # ROLE
        Act as a ruthless IELTS Speaking Examiner and a Phonetic Expert. Your job is to find every single mistake in the user's speech.

        # INPUT DATA
        1. AUDIO: Listen to the attached file for Intonation, Stress, and Pronunciation.
        2. VERBATIM TRANSCRIPT (From Whisper AI): "{transcript}"
           (‚ö†Ô∏è WARNING: This transcript contains EXACTLY what the user said, including fillers like 'um, ah, uh', grammar mistakes, and hesitations).

        # ANALYSIS TASKS:
        1. **Fluency & Coherence:** - Count the fillers (um, ah, uh). If there are many, score LOW.
           - Is the speed natural or robotic/too slow?
        2. **Lexical Resource:** - Are they using basic words (happy, sad, go) or idiomatic language (over the moon, devastating)?
        3. **Grammatical Range:** - Look at the TRANSCRIPT. Identify wrong tenses, wrong prepositions.
        4. **Pronunciation:** - Listen to the Audio. Identify mispronounced words compared to standard IPA.

        # OUTPUT FORMAT (JSON ONLY - Feedback in VIETNAMESE):
        {{
            "transcript_display": "{transcript}",
            "overall_score": "Band [Score]",
            "radar_chart": {{ "Fluency": [Score], "Lexical": [Score], "Grammar": [Score], "Pronunciation": [Score] }},
            "detailed_feedback": {{
                "fluency": "[Nh·∫≠n x√©t th·∫≥ng th·∫Øn v·ªÅ ƒë·ªô tr√¥i ch·∫£y, li·ªát k√™ c√°c t·ª´ ·∫≠m ·ª´]",
                "pronunciation": "[Nh·∫≠n x√©t v·ªÅ ng·ªØ ƒëi·ªáu v√† ph√°t √¢m]",
                "vocab_grammar": "[Nh·∫≠n x√©t v·ªÅ l·ªói ng·ªØ ph√°p v√† t·ª´ v·ª±ng trong Transcript]"
            }},
            "mistakes_timeline": [
                {{
                    "word": "[T·ª´ b·ªã sai/T·ª´ d·ªü]",
                    "error": "[Gi·∫£i th√≠ch t·∫°i sao sai (Grammar/Pronunciation/Choice)]",
                    "fix": "[G·ª£i √Ω s·ª≠a l·∫°i cho chu·∫©n native]"
                }}
            ],
            "vocab_upgrade": [
                {{
                    "original": "[T·ª´ v·ª±ng c∆° b·∫£n user d√πng]",
                    "better": "[T·ª´ v·ª±ng C1/C2 thay th·∫ø]",
                    "reason": "[T·∫°i sao t·ª´ m·ªõi n√†y x·ªãn h∆°n?]"
                }}
            ],
            "better_version": "[Vi·∫øt l·∫°i c√¢u tr·∫£ l·ªùi c·ªßa user theo phong c√°ch Band 9.0 t·ª± nhi√™n]"
        }}
        """

        # Upload file v√† g·ªçi API
        uploaded_file = client.files.upload(path=tmp_path)
        response = client.models.generate_content(
            model=MODEL_NAME,
            contents=[prompt, uploaded_file]
        )
        
        # D·ªçn d·∫πp
        os.remove(tmp_path)
        print(f"‚úÖ X·ª≠ l√Ω xong trong {time.time() - start_time}s")

        return response.text.replace('```json', '').replace('```', '').strip(), 200

    except Exception as e:
        print(f"‚ùå Error: {str(e)}")
        return jsonify({"error": str(e)}), 500

async def generate_audio_edge(text, filepath):
    # Ch·ªçn gi·ªçng: 
    # 'en-GB-SoniaNeural': Gi·ªçng N·ªØ Anh-Anh (Chu·∫©n IELTS)
    # 'en-US-AriaNeural': Gi·ªçng N·ªØ M·ªπ
    # 'en-GB-RyanNeural': Gi·ªçng Nam Anh-Anh
    VOICE = "en-GB-SoniaNeural" 
    communicate = edge_tts.Communicate(text, VOICE)
    await communicate.save(filepath)

@app.route('/api/speaking/conversation', methods=['POST'])
def conversation():
    try:
        # 1. Nh·∫≠n d·ªØ li·ªáu
        if 'audio' not in request.files: return jsonify({"error": "Thi·∫øu audio"}), 400
        
        audio_file = request.files['audio']
        history_str = request.form.get('history', '[]') 
        
        # 2. L∆∞u file t·∫°m & Whisper nghe
        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as tmp:
            audio_file.save(tmp.name)
            tmp_path = tmp.name

        print(f"üó£Ô∏è Conversation: Whisper ƒëang nghe...")
        if whisper_model is None: return jsonify({"error": "Whisper ch∆∞a load!"}), 500
        
        result = whisper_model.transcribe(tmp_path)
        user_text = result["text"]
        print(f"üìù User n√≥i: {user_text}")
        os.remove(tmp_path) 

        # 3. G·ª≠i cho Gemini
        prompt = f"""
        VAI TR√í: B·∫°n l√† m·ªôt Gi√°m kh·∫£o IELTS Speaking chuy√™n nghi·ªáp.
        L·ªäCH S·ª¨: {history_str}
        C√ÇU TR·∫¢ L·ªúI M·ªöI NH·∫§T: "{user_text}"
        
        NHI·ªÜM V·ª§:
        1. Soi l·ªói ng·ªØ ph√°p/t·ª´ v·ª±ng.
        2. T·∫°o c√¢u h·ªèi ti·∫øp theo (Ti·∫øng Anh).
        3. T·∫°o l·ªùi khuy√™n s·ª≠a l·ªói (Ti·∫øng Vi·ªát).

        OUTPUT JSON:
        {{
            "examiner_response_text": "[C√¢u h·ªèi ti·∫øp theo b·∫±ng Ti·∫øng Anh]",
            "correction_tip": "[L·ªùi khuy√™n s·ª≠a l·ªói b·∫±ng Ti·∫øng Vi·ªát]",
            "is_short_answer": true/false
        }}
        """
        
        response = client.models.generate_content(
            model=MODEL_NAME,
            contents=prompt
        )
        response_json = response.text.replace('```json', '').replace('```', '').strip()
        
        # 4. X·ª≠ l√Ω JSON & T·∫°o Audio
        data = json.loads(response_json)
        ai_text = data.get("examiner_response_text", "Could you repeat that?")
        correction = data.get("correction_tip", "")

        # T·∫°o t√™n file
        filename = f"ai_ask_{uuid.uuid4()}.mp3"
        filepath = os.path.join("static", filename)
        
        # G·ªçi h√†m Edge TTS (X·ª≠ l√Ω Async)
        try:
            asyncio.run(generate_audio_edge(ai_text, filepath))
        except Exception as e:
            # Fallback n·∫øu l·ªói loop
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            loop.run_until_complete(generate_audio_edge(ai_text, filepath))
            loop.close()

        audio_url = f"{request.host_url}static/{filename}"
        
        return jsonify({
            "user_transcript": user_text,
            "ai_response_text": ai_text,
            "ai_audio_url": audio_url,
            "correction": correction
        })

    except Exception as e:
        print(f"‚ùå L·ªói Conversation: {e}")
        return jsonify({"error": str(e)}), 500

if __name__ == '__main__':
    app.run(port=5000, debug=True)